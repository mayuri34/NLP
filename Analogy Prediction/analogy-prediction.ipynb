{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analogy Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gensim\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-training Word2Vector and Glove word embeddings from gensim with Google News files\n",
    " Note: For compression file issues, files such as 'GoogleNews-vectors-negative300.bin' and 'glove.840B.300d.txt' haven't been included in this zip. Kindly download them in this folder for running the jupyter code\n",
    "-  https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = gensim.models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True,  limit=500000)\n",
    "glove2word2vec('glove.840B.300d.txt', 'glove.840B.300d.txt.word2vec')\n",
    "glove_model = gensim.models.KeyedVectors.load_word2vec_format('./glove.840B.300d.txt.word2vec', binary=False, limit=500000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Mikolov's analogy test file\n",
    "-  #### Store starting position of all categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = open('word-test.v1.txt', 'r')\n",
    "\n",
    "categories=[]\n",
    "analogy_loc ={}\n",
    "\n",
    "while(True):\n",
    "    line = test_file.readline()\n",
    "    \n",
    "    if not line:\n",
    "        break;\n",
    "        \n",
    "    elif(line.startswith(':')):\n",
    "        category = line[2:].strip()\n",
    "        position = test_file.tell()\n",
    "        analogy_loc[category]= position\n",
    "        categories.append(category)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "### Groups to execute analogy prediction on:\n",
    "-  capital-world\n",
    "-  currency\n",
    "-  city-in-state\n",
    "-  family\n",
    "-  gram1-adjective-to-adverb\n",
    "-  gram2-opposite\n",
    "-  gram3-comparative\n",
    "-  gram6-nationality-adjective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_groups=['capital-world', 'currency', 'city-in-state', 'family', 'gram1-adjective-to-adverb', 'gram2-opposite', 'gram3-comparative', 'gram6-nationality-adjective']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vector Embedding Model\n",
    "-  Feeding first word as negative and next two words as positive to recieve top negative as predicted outcome\n",
    "-  If the predicted word is same as actual word, increment the score of embedding model\n",
    "-  Accuracy calculation per group: correct predictions * 100 / total words\n",
    "-  Average of accuracies from all groups: (overall accuracy / 8)\n",
    "\n",
    "## Glove Embedding Model\n",
    "-  Feeding first word as negative and next two words as positive to recieve top negative as predicted outcome\n",
    "-  If the predicted word is same as actual word, increment the score of embedding model\n",
    "-  Accuracy calculation per group: correct predictions * 100 / total words\n",
    "-  Average of accuracies from all groups: (overall accuracy / 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Word2Vec model for group :  ('capital-world', 78.9787798408488)\n",
      "Accuracy of Glove model for group :  ('capital-world', 90.36251105216623)\n",
      "Accuracy of Word2Vec model for group :  ('currency', 29.907621247113163)\n",
      "Accuracy of Glove model for group :  ('currency', 20.785219399538107)\n",
      "Accuracy of Word2Vec model for group :  ('city-in-state', 71.99027158492096)\n",
      "Accuracy of Glove model for group :  ('city-in-state', 70.08512363194163)\n",
      "Accuracy of Word2Vec model for group :  ('family', 85.17786561264822)\n",
      "Accuracy of Glove model for group :  ('family', 95.8498023715415)\n",
      "Accuracy of Word2Vec model for group :  ('gram1-adjective-to-adverb', 29.233870967741936)\n",
      "Accuracy of Glove model for group :  ('gram1-adjective-to-adverb', 42.84274193548387)\n",
      "Accuracy of Word2Vec model for group :  ('gram2-opposite', 42.98029556650246)\n",
      "Accuracy of Glove model for group :  ('gram2-opposite', 34.35960591133005)\n",
      "Accuracy of Word2Vec model for group :  ('gram3-comparative', 91.14114114114115)\n",
      "Accuracy of Glove model for group :  ('gram3-comparative', 87.68768768768768)\n",
      "Accuracy of Word2Vec model for group :  ('gram6-nationality-adjective', 90.0562851782364)\n",
      "Accuracy of Glove model for group :  ('gram6-nationality-adjective', 90.36898061288305)\n"
     ]
    }
   ],
   "source": [
    "overall_w2v_accuracy = 0\n",
    "overall_glove_accuracy = 0\n",
    "\n",
    "for group in test_groups:\n",
    "    total_words = 0\n",
    "    w2v_score = 0\n",
    "    glove_score = 0 \n",
    "    accuracy = 0\n",
    "    test_file.seek(analogy_loc[group])\n",
    "    \n",
    "    for line in test_file:\n",
    "        if (line.startswith(':')):\n",
    "            break\n",
    "        total_words += 1\n",
    "        line = line.split()\n",
    "        line = [w.strip() for w in line]\n",
    "        \n",
    "        try:\n",
    "            w2v_prediction = w2v_model.most_similar(positive=[line[1], line[2]], negative=[line[0]], topn=1)\n",
    "            glove_prediction = glove_model.most_similar(positive=[line[1], line[2]], negative=[line[0]], topn=1)\n",
    "\n",
    "            if(w2v_prediction[0][0] == line[3]):\n",
    "                w2v_score += 1\n",
    "            if(glove_prediction[0][0] == line[3]):\n",
    "                glove_score += 1\n",
    "            \n",
    "        except Exception as ex:\n",
    "                exception=1\n",
    "\n",
    "    if w2v_score > 0:\n",
    "        accuracy = (w2v_score * 100) / total_words\n",
    "        \n",
    "    print('Accuracy of Word2Vec model for group : ', (group, accuracy))\n",
    "    overall_w2v_accuracy+=accuracy\n",
    "    accuracy = 0\n",
    "    if glove_score > 0:\n",
    "        accuracy = (glove_score * 100) / total_words\n",
    "    print('Accuracy of Glove model for group : ', (group, accuracy))\n",
    "    overall_glove_accuracy+=accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracies:\n",
      "Word2Vec Model:  64.93326639239415\n",
      "Glove Model:  66.54270907532153\n"
     ]
    }
   ],
   "source": [
    "print('Average accuracies:')\n",
    "print('Word2Vec Model: ', overall_w2v_accuracy/8)\n",
    "print('Glove Model: ', overall_glove_accuracy/8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation\n",
    "</br> Glove embedding model has comparatively better prediction rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "### Comparison of antonyms cosine similarities\n",
    "__Word__    __Antonym__ </br>  \n",
    "decrease --- increase </br>  \n",
    "leave --- stay </br>  \n",
    "ascend --- descend </br>  \n",
    "go --- come </br>  \n",
    "above --- below </br>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarities(wrd):\n",
    "    print('\\nTop 10 similar words to : ', wrd)\n",
    "    similar_words = w2v_model.similar_by_word(word=wrd, topn=10)\n",
    "    [print(s) for s in similar_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 similar words to :  decrease\n",
      "('increase', 0.8370319604873657)\n",
      "('decreases', 0.8093847632408142)\n",
      "('decreased', 0.7642107009887695)\n",
      "('reduction', 0.7175438404083252)\n",
      "('increased', 0.7083162069320679)\n",
      "('decreasing', 0.6931016445159912)\n",
      "('decline', 0.6863038539886475)\n",
      "('increases', 0.6454968452453613)\n",
      "('Decreased', 0.574552059173584)\n",
      "('reduced', 0.5725899934768677)\n",
      "\n",
      "Top 10 similar words to :  leave\n",
      "('leaving', 0.6598548889160156)\n",
      "('stay', 0.5787086486816406)\n",
      "('depart', 0.5559219121932983)\n",
      "('Leaving', 0.5488995313644409)\n",
      "('left', 0.5250931978225708)\n",
      "('leaves', 0.5131403803825378)\n",
      "('return', 0.5068632364273071)\n",
      "('vacate', 0.4940752387046814)\n",
      "('quit', 0.4841381311416626)\n",
      "('rejoin', 0.4835888743400574)\n",
      "\n",
      "Top 10 similar words to :  ascend\n",
      "('ascended', 0.718950629234314)\n",
      "('ascending', 0.7094936370849609)\n",
      "('ascends', 0.6623241901397705)\n",
      "('climb', 0.6534912586212158)\n",
      "('ascent', 0.6063636541366577)\n",
      "('descend', 0.5348040461540222)\n",
      "('ascension', 0.5295450687408447)\n",
      "('clamber', 0.5279775261878967)\n",
      "('descending', 0.5162654519081116)\n",
      "('Ascending', 0.5003010034561157)\n",
      "\n",
      "Top 10 similar words to :  go\n",
      "('come', 0.660467803478241)\n",
      "('goes', 0.6306792497634888)\n",
      "('gone', 0.6219984889030457)\n",
      "('going', 0.5955760478973389)\n",
      "('get', 0.5898032188415527)\n",
      "('went', 0.5646423697471619)\n",
      "('do', 0.5437604188919067)\n",
      "('sit', 0.5383905172348022)\n",
      "('stay', 0.5288218855857849)\n",
      "('Going', 0.5277284979820251)\n",
      "\n",
      "Top 10 similar words to :  above\n",
      "('below', 0.8064708709716797)\n",
      "('Above', 0.5198284387588501)\n",
      "('beyond', 0.4812318980693817)\n",
      "('beneath', 0.45852547883987427)\n",
      "('Immediate_resistance', 0.4387754797935486)\n",
      "('hover', 0.4326574206352234)\n",
      "('hovering', 0.42778879404067993)\n",
      "('elevated', 0.42664292454719543)\n",
      "('skyward', 0.4200187921524048)\n",
      "('exceeds', 0.419230580329895)\n"
     ]
    }
   ],
   "source": [
    "verbs = ['decrease', 'leave', 'ascend', 'go', 'above']\n",
    "for v in verbs:\n",
    "    cosine_similarities(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Word__    __Antonym__   __Similarity__ </br>  \n",
    "-  decrease --- increase --- 83% </br>  \n",
    "-  leave --- stay --- 53% </br>  \n",
    "-  ascend --- descend--- 53% </br>  \n",
    "-  go --- come --- 66% </br>  \n",
    "-  above --- below --- 80% </br>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Why are word embeddings similar for antonyms?__  \n",
    "</br> Based on above output, we can see that often opposite words are listed as top ten similar words based on their word embeddings. This could happen because even though these terms have opposite meanings, they hold a similar context. Words like 'decrease' and 'increase', 'leave' and 'stay' appear in the same sentence more frequently. Hence, inspite of being semantically diverse, such words are still found to be similar based on frequency and statistics of a corpora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "### Prediction on a Custom-made Test\n",
    "\n",
    "### Category 1 (Animal Sounds)\n",
    "-  duck quack dog bark\n",
    "-  duck quack bat screech\n",
    "-  duck quack dolphin click\n",
    "\n",
    "### Category 2 (Owner Company - AI Products)\n",
    "-  apple siri google google_assistant\n",
    "-  apple siri amazon alexa\n",
    "-  apple siri microsoft cortana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_test_file = open('custom-test.txt', 'r')\n",
    "\n",
    "custom_categories=[]\n",
    "custom_analogy_loc ={}\n",
    "\n",
    "while(True):\n",
    "    \n",
    "    line = custom_test_file.readline()\n",
    "    if not line:\n",
    "        break;\n",
    "        \n",
    "    elif(line.startswith(':')):\n",
    "        custom_analogy_loc[line[2:].strip()]= custom_test_file.tell()\n",
    "        custom_categories.append(line[2:].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Feeding first word as negative and next two words as positive to recieve top negative as predicted outcome\n",
    "-  If the predicted word is same as actual word, increment the score of respective embedding models\n",
    "-  Accuracy calculation per group: correct predictions * 100 / total words\n",
    "-  Average of accuracies from all groups: (overall accuracy / 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Group:  animal-sounds\n",
      "duck quack dog bark Word2Vec Prediction:  dogs Glove Prediction:  puppy\n",
      "duck quack bat screech Word2Vec Prediction:  bats Glove Prediction:  bats\n",
      "duck quack dolphin click Word2Vec Prediction:  dolphins Glove Prediction:  dolphins\n",
      "\n",
      "Accuracy of Word2Vec model for group :  ('animal-sounds', 0)\n",
      "Accuracy of Glove model for group :  ('animal-sounds', 0)\n",
      "\n",
      "Group:  company-AI_product\n",
      "\"word 'siri' not in vocabulary\"\n",
      "\"word 'siri' not in vocabulary\"\n",
      "\"word 'siri' not in vocabulary\"\n",
      "\n",
      "Accuracy of Word2Vec model for group :  ('company-AI_product', 0)\n",
      "Accuracy of Glove model for group :  ('company-AI_product', 0)\n"
     ]
    }
   ],
   "source": [
    "custom_test_groups=['animal-sounds', 'company-AI_product']\n",
    "\n",
    "for group in custom_test_groups:\n",
    "    total_words = 0\n",
    "    w2v_score = 0\n",
    "    glove_score = 0 \n",
    "    accuracy = 0\n",
    "    custom_test_file.seek(custom_analogy_loc[group])\n",
    "    print('\\nGroup: ', group)\n",
    "    for line in custom_test_file:\n",
    "        if (line.startswith(':')):\n",
    "            break\n",
    "        total_words += 1\n",
    "        line = line.split()\n",
    "        line = [w.strip() for w in line]\n",
    "        \n",
    "        try:\n",
    "            w2v_prediction = w2v_model.most_similar(positive=[line[1], line[2]], negative=[line[0]], topn=1)\n",
    "            print(line[0], line[1], line[2], line[3], end =\" \")\n",
    "            print('Word2Vec Prediction: ', w2v_prediction[0][0], end =\" \")\n",
    "            glove_prediction = glove_model.most_similar(positive=[line[1], line[2]], negative=[line[0]], topn=1)\n",
    "            \n",
    "            print('Glove Prediction: ', glove_prediction[0][0])\n",
    "            if(w2v_prediction[0][0] == line[3]):\n",
    "                w2v_score += 1\n",
    "            if(glove_prediction[0][0] == line[3]):\n",
    "                glove_score += 1\n",
    "            \n",
    "        except Exception as ex:\n",
    "                print(ex)\n",
    "\n",
    "    if w2v_score > 0:\n",
    "        accuracy = (w2v_score * 100) / total_words\n",
    "        \n",
    "    print('\\nAccuracy of Word2Vec model for group : ', (group, accuracy))\n",
    "    accuracy = 0\n",
    "    if glove_score > 0:\n",
    "        accuracy = (glove_score * 100) / total_words\n",
    "    print('Accuracy of Glove model for group : ', (group, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation on results:\n",
    "-  In the first set of analogies, words are known to the pretrained word embedding models, but in a different context. Hence, we can see its failed attempts at predicting the fourt word.\n",
    "-  The second category set has words completely unknown to word embedding models like 'siri'. Therefore, its unable to proceed with any predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
